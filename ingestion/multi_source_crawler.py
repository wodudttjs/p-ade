"""
ë‹¤ì¤‘ í”Œë«í¼ ë³‘ë ¬ í¬ë¡¤ëŸ¬

YouTube, Google Videos, Vimeo, Dailymotionì—ì„œ ë³‘ë ¬ë¡œ ì˜ìƒì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
- ì†ŒìŠ¤ë³„ ë…ë¦½ ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…
- ë³‘ë ¬ í¬ë¡¤ë§ (ThreadPoolExecutor)
- ì¤‘ë³µ URL/video_id ì œê±°
- í’ˆì§ˆ/ê´€ë ¨ì„± ì‚¬ì „ í•„í„°ë§
- ì§„í–‰ë¥  ì½œë°± & í†µê³„ ë¦¬í¬íŠ¸
- DB ë™ê¸°í™” (Video/ProcessingJob upsert)
"""

import csv
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Optional, Callable, Set, Any
from dataclasses import dataclass, field, asdict

import yt_dlp
import requests
import re
from urllib.parse import quote_plus

from core.logging_config import setup_logger
from ingestion.rate_limiter import SourceRateLimiter, RetryManager, RetryConfig

logger = setup_logger(__name__)


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""
    video_id: str
    url: str
    title: str = ""
    description: str = ""
    duration_sec: Optional[int] = None
    view_count: Optional[int] = None
    channel_id: str = ""
    channel_name: str = ""
    thumbnail_url: str = ""
    tags: List[str] = field(default_factory=list)
    platform: str = ""
    keyword: str = ""
    discovered_at: str = ""


@dataclass
class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„"""
    total_searched: int = 0
    total_found: int = 0
    total_filtered: int = 0
    total_duplicates: int = 0
    total_errors: int = 0
    by_source: Dict[str, int] = field(default_factory=dict)
    by_keyword: Dict[str, int] = field(default_factory=dict)
    elapsed_sec: float = 0.0

    def summary(self) -> str:
        lines = [
            f"{'='*60}",
            f"ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½",
            f"{'='*60}",
            f"  ì´ ê²€ìƒ‰: {self.total_searched}ê°œ",
            f"  ì´ ë°œê²¬: {self.total_found}ê°œ",
            f"  ì¤‘ë³µ ì œê±°: {self.total_duplicates}ê°œ",
            f"  í•„í„°ë§ë¨: {self.total_filtered}ê°œ",
            f"  ì—ëŸ¬: {self.total_errors}ê°œ",
            f"  ì†Œìš” ì‹œê°„: {self.elapsed_sec:.1f}ì´ˆ",
        ]
        if self.by_source:
            lines.append(f"\n  ì†ŒìŠ¤ë³„:")
            for src, cnt in sorted(self.by_source.items()):
                lines.append(f"    {src}: {cnt}ê°œ")
        return "\n".join(lines)


# ============================================================
# í•„í„° ì„¤ì •
# ============================================================

# ê´€ë ¨ì„± í‚¤ì›Œë“œ (ì œëª©/ì„¤ëª…ì— í¬í•¨ë˜ì–´ì•¼ í•¨)
RELEVANCE_KEYWORDS = [
    "robot", "robotic", "arm", "gripper", "manipulator",
    "pick", "place", "grasping", "cobot", "industrial",
    "FANUC", "ABB", "KUKA", "UR5", "UR10", "UR3",
    "automation", "manufacturing", "assembly",
    "ë¡œë´‡", "ë¡œë´‡íŒ”", "ê·¸ë¦¬í¼", "ë§¤ë‹ˆí“°ë ˆì´í„°",
]

# ê±°ë¶€ í‚¤ì›Œë“œ
REJECT_KEYWORDS = [
    "animation", "cartoon", "cgi", "3d render", "toy",
    "lego", "surgery", "medical", "prosthetic", "game",
    "minecraft", "roblox", "fortnite",
    "unboxing", "review", "price", "how much",
]

# ê¸¸ì´ í•„í„°
MIN_DURATION_SEC = 30
MAX_DURATION_SEC = 1200  # 20ë¶„


def _passes_content_filter(title: str, description: str = "") -> bool:
    """ì½˜í…ì¸  ê´€ë ¨ì„± í•„í„°"""
    text = f"{title} {description}".lower()

    # ê±°ë¶€ í‚¤ì›Œë“œ ì²´í¬
    for rk in REJECT_KEYWORDS:
        if rk in text:
            return False

    # ê´€ë ¨ì„± í‚¤ì›Œë“œ ì²´í¬ (ìµœì†Œ 1ê°œ)
    for kw in RELEVANCE_KEYWORDS:
        if kw.lower() in text:
            return True

    return False


def _passes_duration_filter(
    duration: Optional[int],
    min_sec: int = MIN_DURATION_SEC,
    max_sec: int = MAX_DURATION_SEC,
) -> bool:
    """ê¸¸ì´ í•„í„°"""
    if duration is None:
        return True  # ê¸¸ì´ ì •ë³´ ì—†ìœ¼ë©´ í†µê³¼ (ë‚˜ì¤‘ì— ë‹¤ìš´ë¡œë“œ ì‹œ í•„í„°)
    return min_sec <= duration <= max_sec


# ============================================================
# ì†ŒìŠ¤ë³„ í¬ë¡¤ëŸ¬
# ============================================================

class _YtDlpSearcher:
    """yt-dlp ê¸°ë°˜ ê²€ìƒ‰ê¸° (YouTube, Google Videos)"""

    EXTRACTOR_MAP = {
        "youtube": "ytsearch",
        "google_videos": "gvsearch",
    }

    def __init__(self, rate_limiter: SourceRateLimiter):
        self._limiter = rate_limiter
        self._retry = RetryManager(RetryConfig(max_retries=3))

    def search(
        self,
        source: str,
        keyword: str,
        max_results: int = 50,
        get_full_info: bool = False,
    ) -> List[CrawlResult]:
        """yt-dlp ê²€ìƒ‰ ì‹¤í–‰"""
        extractor = self.EXTRACTOR_MAP.get(source)
        if not extractor:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤: {source}")
            return []

        self._limiter.wait_for(source)

        query = f"{extractor}{max_results}:{keyword}"
        ydl_opts = {"quiet": True, "no_warnings": True, "extract_flat": True}
        results: List[CrawlResult] = []

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                raw = ydl.extract_info(query, download=False)

            if not raw or "entries" not in raw:
                return []

            for entry in raw.get("entries", []):
                if not entry:
                    continue

                duration = entry.get("duration")

                # flat ëª¨ë“œì—ì„œ durationì´ ì—†ìœ¼ë©´ ìƒì„¸ ì •ë³´ ìš”ì²­
                if get_full_info and duration is None:
                    self._limiter.wait_for(source)
                    try:
                        with yt_dlp.YoutubeDL(
                            {"quiet": True, "no_warnings": True, "skip_download": True}
                        ) as ydl2:
                            full = ydl2.extract_info(
                                entry.get("url") or entry.get("webpage_url"),
                                download=False,
                            )
                            if full:
                                duration = full.get("duration")
                                entry.update({
                                    k: v for k, v in full.items()
                                    if k in ("description", "view_count", "channel_id",
                                             "uploader", "thumbnail", "tags", "duration")
                                })
                    except Exception:
                        pass

                cr = CrawlResult(
                    video_id=entry.get("id", ""),
                    url=entry.get("url") or entry.get("webpage_url") or "",
                    title=entry.get("title", ""),
                    description=entry.get("description", ""),
                    duration_sec=duration,
                    view_count=entry.get("view_count"),
                    channel_id=entry.get("channel_id", ""),
                    channel_name=entry.get("uploader", ""),
                    thumbnail_url=entry.get("thumbnail", ""),
                    tags=entry.get("tags", []) or [],
                    platform=source,
                    keyword=keyword,
                    discovered_at=datetime.now(timezone.utc).isoformat(),
                )
                results.append(cr)

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ [{source}] '{keyword}': {e}")

        return results


class _SiteScraper:
    """ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ê¸°ë°˜ ê²€ìƒ‰ê¸° (Vimeo, Dailymotion)"""

    def __init__(self, rate_limiter: SourceRateLimiter):
        self._limiter = rate_limiter

    def search(
        self,
        source: str,
        keyword: str,
        max_results: int = 20,
    ) -> List[CrawlResult]:
        """ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ê²€ìƒ‰"""
        self._limiter.wait_for(source)

        urls = self._scrape_urls(source, keyword, max_results)
        results: List[CrawlResult] = []

        for url in urls:
            self._limiter.wait_for(source)
            try:
                with yt_dlp.YoutubeDL(
                    {"quiet": True, "no_warnings": True, "skip_download": True}
                ) as ydl:
                    info = ydl.extract_info(url, download=False)

                if not info:
                    continue

                cr = CrawlResult(
                    video_id=info.get("id", ""),
                    url=info.get("webpage_url") or url,
                    title=info.get("title", ""),
                    description=info.get("description", ""),
                    duration_sec=info.get("duration"),
                    view_count=info.get("view_count"),
                    channel_id=info.get("channel_id", ""),
                    channel_name=info.get("uploader") or info.get("uploader_id", ""),
                    thumbnail_url=info.get("thumbnail", ""),
                    tags=info.get("tags", []) or [],
                    platform=source,
                    keyword=keyword,
                    discovered_at=datetime.now(timezone.utc).isoformat(),
                )
                results.append(cr)
            except Exception as e:
                logger.debug(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ [{source}] {url}: {e}")

        return results

    def _scrape_urls(self, source: str, keyword: str, limit: int) -> List[str]:
        """ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ URL ìŠ¤í¬ë˜í•‘"""
        urls: List[str] = []
        try:
            if source == "vimeo":
                search_url = f"https://vimeo.com/search?q={quote_plus(keyword)}"
                pattern = r'https?://vimeo\.com/\d+'
            elif source == "dailymotion":
                search_url = f"https://www.dailymotion.com/search/{quote_plus(keyword)}/videos"
                pattern = r'https?://www\.dailymotion\.com/video/[a-zA-Z0-9]+'
            elif source == "bilibili":
                search_url = f"https://search.bilibili.com/all?keyword={quote_plus(keyword)}"
                pattern = r'https?://www\.bilibili\.com/video/[A-Za-z0-9]+'
            elif source == "rutube":
                search_url = f"https://rutube.ru/api/search/video/?query={quote_plus(keyword)}"
                pattern = r'https?://rutube\.ru/video/[a-f0-9]+'
            else:
                return []

            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
            }
            resp = requests.get(search_url, headers=headers, timeout=15)
            found = re.findall(pattern, resp.text)

            seen: Set[str] = set()
            for u in found:
                u = u.rstrip("/")
                if u not in seen:
                    seen.add(u)
                    urls.append(u)
                if len(urls) >= limit:
                    break
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ [{source}] '{keyword}': {e}")

        return urls


# ============================================================
# ë‹¤ì¤‘ ì†ŒìŠ¤ ë³‘ë ¬ í¬ë¡¤ëŸ¬
# ============================================================

class MultiSourceCrawler:
    """ë‹¤ì¤‘ í”Œë«í¼ ë³‘ë ¬ í¬ë¡¤ëŸ¬

    ì‚¬ìš©ë²•:
        crawler = MultiSourceCrawler(
            sources=["youtube", "google_videos", "vimeo"],
            max_results=500,
            max_workers=4,
        )
        results, stats = crawler.crawl(keywords=["robot arm", "pick and place"])
    """

    SUPPORTED_SOURCES = ["youtube", "google_videos", "vimeo", "dailymotion", "bilibili", "rutube"]

    def __init__(
        self,
        sources: Optional[List[str]] = None,
        max_results: int = 500,
        max_workers: int = 4,
        get_full_info: bool = False,
        min_duration_sec: int = MIN_DURATION_SEC,
        max_duration_sec: int = MAX_DURATION_SEC,
        content_filter: bool = True,
    ):
        self.sources = sources or ["youtube", "google_videos"]
        self.max_results = max_results
        self.max_workers = max_workers
        self.get_full_info = get_full_info
        self.min_duration_sec = min_duration_sec
        self.max_duration_sec = max_duration_sec
        self.content_filter = content_filter

        self._rate_limiter = SourceRateLimiter()
        self._ydl_searcher = _YtDlpSearcher(self._rate_limiter)
        self._site_scraper = _SiteScraper(self._rate_limiter)

        self._seen_ids: Set[str] = set()
        self._lock = threading.Lock()

    def crawl(
        self,
        keywords: List[str],
        progress_callback: Optional[Callable[[int, int], None]] = None,
    ) -> tuple:  # (List[CrawlResult], CrawlStats)
        """
        ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰

        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ë¥  ì½œë°± (í˜„ì¬, ì „ì²´)

        Returns:
            (ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, í†µê³„ ê°ì²´)
        """
        start_time = time.time()
        stats = CrawlStats()
        all_results: List[CrawlResult] = []

        # ì‘ì—… ëª©ë¡ ìƒì„±: (source, keyword, per_task_limit)
        tasks = []
        per_task = max(1, self.max_results // max(1, len(self.sources) * len(keywords)))

        for source in self.sources:
            if source not in self.SUPPORTED_SOURCES:
                logger.warning(f"ë¯¸ì§€ì› ì†ŒìŠ¤: {source}")
                continue
            for kw in keywords:
                tasks.append((source, kw, per_task))

        total_tasks = len(tasks)
        completed_tasks = 0

        logger.info(
            f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ Ã— {len(self.sources)}ê°œ ì†ŒìŠ¤ = {total_tasks}ê°œ ì‘ì—…"
        )

        # ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰
        # yt-dlp ê¸°ë°˜ê³¼ scraper ê¸°ë°˜ì„ ë¶„ë¦¬
        ydl_tasks = [(s, kw, lim) for s, kw, lim in tasks if s in ("youtube", "google_videos")]
        scrape_tasks = [(s, kw, lim) for s, kw, lim in tasks if s in ("vimeo", "dailymotion", "bilibili", "rutube")]

        # yt-dlp íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰ (ì†ŒìŠ¤ë³„ ë ˆì´íŠ¸ ë¦¬ë°‹ ì ìš©ë˜ë¯€ë¡œ ì›Œì»¤ ìˆ˜ ì œí•œ)
        ydl_workers = min(self.max_workers, len(ydl_tasks)) if ydl_tasks else 0
        scrape_workers = min(2, len(scrape_tasks)) if scrape_tasks else 0

        def _execute_task(task_info):
            source, kw, limit = task_info
            if source in ("youtube", "google_videos"):
                return self._ydl_searcher.search(source, kw, limit, self.get_full_info)
            else:
                return self._site_scraper.search(source, kw, limit)

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max(1, ydl_workers + scrape_workers)) as executor:
            future_map = {
                executor.submit(_execute_task, t): t for t in tasks
            }

            for future in as_completed(future_map):
                task_info = future_map[future]
                source, kw, _ = task_info
                completed_tasks += 1

                try:
                    results = future.result()
                    stats.total_searched += 1

                    for cr in results:
                        # ì¤‘ë³µ ì²´í¬
                        with self._lock:
                            if cr.video_id in self._seen_ids:
                                stats.total_duplicates += 1
                                continue
                            self._seen_ids.add(cr.video_id)

                        # ì½˜í…ì¸  í•„í„°
                        if self.content_filter:
                            if not _passes_content_filter(cr.title, cr.description):
                                stats.total_filtered += 1
                                continue

                        # ê¸¸ì´ í•„í„°
                        if not _passes_duration_filter(
                            cr.duration_sec,
                            self.min_duration_sec,
                            self.max_duration_sec,
                        ):
                            stats.total_filtered += 1
                            continue

                        all_results.append(cr)
                        stats.total_found += 1

                        # ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸
                        stats.by_source[source] = stats.by_source.get(source, 0) + 1
                        stats.by_keyword[kw] = stats.by_keyword.get(kw, 0) + 1

                        # ëª©í‘œ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                        if stats.total_found >= self.max_results:
                            break

                except Exception as e:
                    stats.total_errors += 1
                    logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨ [{source}] '{kw}': {e}")

                if progress_callback:
                    progress_callback(completed_tasks, total_tasks)

                # ëª©í‘œ ë„ë‹¬ ì‹œ
                if stats.total_found >= self.max_results:
                    break

        stats.elapsed_sec = time.time() - start_time
        logger.info(stats.summary())

        return all_results, stats

    def save_csv(
        self,
        results: List[CrawlResult],
        output_path: Path,
        overwrite: bool = True,
    ):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        mode = "w" if overwrite else "a"

        fieldnames = [
            "url", "video_id", "title", "platform", "keyword",
            "duration_sec", "view_count", "channel_name", "discovered_at",
        ]

        with output_path.open(mode, encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            if mode == "w":
                writer.writeheader()
            for cr in results:
                writer.writerow({fn: getattr(cr, fn, "") for fn in fieldnames})

        logger.info(f"ğŸ’¾ {len(results)}ê°œ ê²°ê³¼ ì €ì¥: {output_path}")

    def save_to_db(self, results: List[CrawlResult], db_path: str = "data/pade.db"):
        """ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        try:
            from sqlalchemy import create_engine
            from sqlalchemy.orm import sessionmaker
            from models.database import Base, Video, ProcessingJob

            db_file = Path(db_path)
            if not db_file.is_absolute():
                project_root = Path(__file__).resolve().parent.parent
                db_file = project_root / db_file

            engine = create_engine(f"sqlite:///{db_file}")
            Base.metadata.create_all(engine)
            Session = sessionmaker(bind=engine)
            session = Session()

            saved = 0
            for cr in results:
                if not cr.video_id:
                    continue

                existing = session.query(Video).filter_by(video_id=cr.video_id).first()
                if existing:
                    continue

                video = Video(
                    video_id=cr.video_id,
                    platform=cr.platform or "unknown",
                    url=cr.url,
                    title=cr.title,
                    description=cr.description[:2000] if cr.description else None,
                    duration_sec=cr.duration_sec,
                    channel_id=cr.channel_id,
                    channel_name=cr.channel_name,
                    view_count=cr.view_count,
                    thumbnail_url=cr.thumbnail_url,
                    tags=cr.tags,
                    status="discovered",
                    discovered_at=datetime.now(timezone.utc),
                )
                session.add(video)
                saved += 1

                # ProcessingJob
                processing_version = "mass_collect_v1"
                job_key = ProcessingJob.generate_job_key(
                    video.platform, cr.video_id, processing_version
                )
                job = ProcessingJob(
                    job_key=job_key,
                    platform=video.platform,
                    video_id=cr.video_id,
                    processing_version=processing_version,
                    stage="discover",
                    status="completed",
                    started_at=datetime.now(timezone.utc),
                    completed_at=datetime.now(timezone.utc),
                )
                session.add(job)

            session.commit()
            session.close()
            logger.info(f"ğŸ’¾ DBì— {saved}ê°œ ì‹ ê·œ ë¹„ë””ì˜¤ ì €ì¥")
            return saved

        except Exception as e:
            logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0


# ============================================================
# CLI
# ============================================================

def main():
    import argparse

    parser = argparse.ArgumentParser(description="ë‹¤ì¤‘ ì†ŒìŠ¤ ë³‘ë ¬ í¬ë¡¤ëŸ¬")
    parser.add_argument(
        "--keywords", required=True,
        help="ê²€ìƒ‰ í‚¤ì›Œë“œ (ì½¤ë§ˆ êµ¬ë¶„)"
    )
    parser.add_argument(
        "--sources", default="youtube,google_videos",
        help="ì†ŒìŠ¤ ëª©ë¡ (ì½¤ë§ˆ êµ¬ë¶„: youtube,google_videos,vimeo,dailymotion)"
    )
    parser.add_argument("--max-results", type=int, default=500)
    parser.add_argument("--workers", type=int, default=4)
    parser.add_argument("--output", default="data/urls_mass.csv")
    parser.add_argument("--db", default="data/pade.db")
    parser.add_argument("--min-duration", type=int, default=30)
    parser.add_argument("--max-duration", type=int, default=1200)
    parser.add_argument("--full-info", action="store_true")
    parser.add_argument("--no-filter", action="store_true")

    args = parser.parse_args()

    keywords = [k.strip() for k in args.keywords.split(",") if k.strip()]
    sources = [s.strip() for s in args.sources.split(",") if s.strip()]

    crawler = MultiSourceCrawler(
        sources=sources,
        max_results=args.max_results,
        max_workers=args.workers,
        get_full_info=args.full_info,
        min_duration_sec=args.min_duration,
        max_duration_sec=args.max_duration,
        content_filter=not args.no_filter,
    )

    results, stats = crawler.crawl(keywords)

    if results:
        crawler.save_csv(results, Path(args.output))
        crawler.save_to_db(results, args.db)

    print(stats.summary())


if __name__ == "__main__":
    main()
