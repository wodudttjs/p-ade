#!/usr/bin/env python
"""
P-ADE S3 ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

í¬ì¦ˆ ë°ì´í„°(.npz)ì™€ ì—í”¼ì†Œë“œë¥¼ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

Usage:
    python upload_to_s3.py --all                      # ëª¨ë“  í¬ì¦ˆ íŒŒì¼ ì—…ë¡œë“œ
    python upload_to_s3.py --file data/poses/xxx.npz  # íŠ¹ì • íŒŒì¼ ì—…ë¡œë“œ
    python upload_to_s3.py --input data/episodes/     # íŠ¹ì • ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ
    python upload_to_s3.py --dry-run --all            # ì—…ë¡œë“œí•  íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.logging_config import setup_logger
from config.settings import Config

logger = setup_logger(__name__)


def get_bucket_name() -> str:
    """S3 ë²„í‚· ì´ë¦„ ê°€ì ¸ì˜¤ê¸°"""
    return os.getenv("S3_BUCKET", Config.AWS_S3_BUCKET)


def get_s3_provider():
    """S3 Provider ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from storage.providers.s3_provider import S3Provider
    
    return S3Provider(
        region=os.getenv("AWS_REGION", "ap-northeast-2"),
        access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        endpoint_url=os.getenv("S3_ENDPOINT_URL"),  # LocalStack ë“±
    )


def find_pose_files() -> List[Path]:
    """ì—…ë¡œë“œí•  í¬ì¦ˆ íŒŒì¼ ì°¾ê¸°"""
    poses_dir = project_root / "data" / "poses"
    if not poses_dir.exists():
        return []
    return list(poses_dir.glob("*.npz"))


def generate_s3_key(local_path: Path, prefix: str = "poses") -> str:
    """S3 í‚¤ ìƒì„±
    
    í˜•ì‹: poses/YYYY/MM/DD/{video_id}_pose.npz
    """
    today = datetime.now()
    date_prefix = f"{today.year}/{today.month:02d}/{today.day:02d}"
    return f"{prefix}/{date_prefix}/{local_path.name}"


def get_file_metadata(local_path: Path, data_type: str = "pose") -> Dict[str, str]:
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„±"""
    stat = local_path.stat()
    return {
        "original_filename": local_path.name,
        "upload_timestamp": datetime.now().isoformat(),
        "file_size": str(stat.st_size),
        "project": "p-ade",
        "data_type": data_type,
    }


def upload_file(
    provider,
    local_path: Path,
    bucket: str,
    dry_run: bool = False,
    prefix: str = "poses",
    data_type: str = "pose",
) -> Dict:
    """ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ"""
    # íŒŒì¼ ì¡´ìž¬ í™•ì¸
    if not local_path.exists():
        logger.error(f"âŒ íŒŒì¼ ì—†ìŒ: {local_path}")
        return {
            "local_path": str(local_path),
            "status": "error",
            "error": f"File not found: {local_path}",
        }
    
    s3_key = generate_s3_key(local_path, prefix=prefix)
    metadata = get_file_metadata(local_path, data_type=data_type)
    file_size = local_path.stat().st_size
    
    result = {
        "local_path": str(local_path),
        "s3_key": s3_key,
        "bucket": bucket,
        "size_bytes": file_size,
        "size_mb": round(file_size / (1024 * 1024), 2),
    }
    
    if dry_run:
        result["status"] = "dry_run"
        result["uri"] = f"s3://{bucket}/{s3_key}"
        logger.info(f"[DRY-RUN] ì—…ë¡œë“œ ì˜ˆì •: {local_path.name} -> s3://{bucket}/{s3_key}")
        return result
    
    try:
        upload_result = provider.upload_file(
            local_path=str(local_path),
            remote_key=s3_key,
            bucket=bucket,
            metadata=metadata,
            storage_class="STANDARD",
        )
        
        result["status"] = upload_result.status.value
        result["uri"] = upload_result.uri
        result["etag"] = upload_result.etag
        result["sha256"] = upload_result.sha256
        
        if upload_result.status.value == "completed":
            logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {local_path.name} -> {upload_result.uri}")
        elif upload_result.status.value == "skipped":
            logger.info(f"â­ï¸ ì´ë¯¸ ì¡´ìž¬ (skip): {local_path.name}")
        else:
            logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {local_path.name} - {upload_result.error_message}")
            result["error"] = upload_result.error_message
            
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        logger.error(f"âŒ ì—…ë¡œë“œ ì˜¤ë¥˜: {local_path.name} - {e}")
        
    return result


def _parse_episode_ids(file_path: Path) -> Dict[str, str]:
    stem = file_path.stem
    base = stem[:-5] if stem.endswith("_pose") else stem
    if "_ep" in base:
        video_id = base.split("_ep")[0]
        episode_id = base
    else:
        video_id = base
        episode_id = f"{base}_ep001"
    return {"video_id": video_id, "episode_id": episode_id}


def register_episodes_in_db(files: List[Path]):
    """episodes íŒŒì¼ì„ DBì— ë“±ë¡ (local_path, filesize ì—…ë°ì´íŠ¸)"""
    try:
        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker
        from models.database import Base, Video, Episode
    except Exception as e:
        logger.warning(f"DB ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨, ë“±ë¡ ìŠ¤í‚µ: {e}")
        return

    db_path = project_root / "data" / "pade.db"
    if not db_path.exists():
        logger.warning("DB íŒŒì¼ ì—†ìŒ, ë“±ë¡ ìŠ¤í‚µ")
        return

    engine = create_engine(f"sqlite:///{db_path}")
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        for file_path in files:
            ids = _parse_episode_ids(file_path)
            video = session.query(Video).filter_by(video_id=ids["video_id"]).first()
            if not video:
                video = Video(
                    video_id=ids["video_id"],
                    platform="youtube",
                    url="",
                    status="processed",
                )
                session.add(video)
                session.flush()

            episode = session.query(Episode).filter_by(episode_id=ids["episode_id"]).first()
            if not episode:
                episode = Episode(
                    episode_id=ids["episode_id"],
                    video_id=video.id,
                )
                session.add(episode)

            episode.local_path = str(file_path)
            if file_path.exists():
                episode.filesize_bytes = file_path.stat().st_size

        session.commit()
    finally:
        session.close()


def update_database(results: List[Dict]):
    """ì—…ë¡œë“œ ê²°ê³¼ë¥¼ DBì— ë°˜ì˜"""
    try:
        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker
        from models.database import Base, Video, Episode
    except Exception as e:
        logger.warning(f"DB ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ: {e}")
        return

    db_path = project_root / "data" / "pade.db"
    if not db_path.exists():
        logger.warning("DB íŒŒì¼ ì—†ìŒ, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
        return

    engine = create_engine(f"sqlite:///{db_path}")
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()

    updated = 0
    try:
        for result in results:
            if result.get("status") in ["completed", "skipped"]:
                file_path = Path(result["local_path"])
                ids = _parse_episode_ids(file_path)

                video = session.query(Video).filter_by(video_id=ids["video_id"]).first()
                if not video:
                    video = Video(
                        video_id=ids["video_id"],
                        platform="youtube",
                        url="",
                        status="processed",
                    )
                    session.add(video)
                    session.flush()

                episode = session.query(Episode).filter_by(episode_id=ids["episode_id"]).first()
                if not episode:
                    episode = Episode(
                        episode_id=ids["episode_id"],
                        video_id=video.id,
                    )
                    session.add(episode)

                episode.cloud_path = result.get("uri")
                episode.uploaded_at = datetime.now()
                updated += 1

        session.commit()
    finally:
        session.close()

    logger.info(f"ðŸ“ DB ì—…ë°ì´íŠ¸: {updated}ê°œ ì—í”¼ì†Œë“œ")


def main():
    parser = argparse.ArgumentParser(description="P-ADE S3 ì—…ë¡œë“œ")
    parser.add_argument("--all", action="store_true", help="ëª¨ë“  í¬ì¦ˆ íŒŒì¼ ì—…ë¡œë“œ")
    parser.add_argument("--file", type=str, help="íŠ¹ì • íŒŒì¼ ì—…ë¡œë“œ")
    parser.add_argument("--input", type=str, help="íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°")
    parser.add_argument("--bucket", type=str, help="S3 ë²„í‚· ì´ë¦„ (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜)")
    parser.add_argument("--no-db-update", action="store_true", help="DB ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
    parser.add_argument("--prefix", type=str, help="S3 í‚¤ ì ‘ë‘ì–´ (ê¸°ë³¸: ìž…ë ¥ í´ë”ëª…)")
    
    args = parser.parse_args()
    
    if not args.all and not args.file and not args.input:
        parser.print_help()
        print("\nâŒ --all, --file ë˜ëŠ” --input ì˜µì…˜ì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
        
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    prefix = "poses"
    data_type = "pose"

    if args.all:
        files = find_pose_files()
        if not files:
            print("ðŸ“ ì—…ë¡œë“œí•  í¬ì¦ˆ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(0)
    elif args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ ê²½ë¡œ ì—†ìŒ: {args.input}")
            sys.exit(1)
        if input_path.is_file():
            files = [input_path]
        else:
            files = [p for p in input_path.rglob("*") if p.is_file()]
        if not files:
            print(f"ðŸ“ ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.input}")
            sys.exit(0)
        if args.prefix:
            prefix = args.prefix
        else:
            prefix = input_path.name or "input"
        data_type = prefix
    else:
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {args.file}")
            sys.exit(1)
        files = [file_path]
        if args.prefix:
            prefix = args.prefix
        else:
            prefix = file_path.parent.name if file_path.parent.name else "poses"
        data_type = prefix
        
    # ë²„í‚· ì´ë¦„
    bucket = args.bucket or get_bucket_name()
    
    print("=" * 60)
    print("ðŸš€ P-ADE S3 ì—…ë¡œë“œ")
    print("=" * 60)
    print(f"ðŸ“ ì—…ë¡œë“œ íŒŒì¼: {len(files)}ê°œ")
    print(f"ðŸª£ ë²„í‚·: {bucket}")
    print(f"ðŸ”§ Dry-run: {args.dry_run}")
    print()
    
    # S3 Provider ì´ˆê¸°í™”
    if not args.dry_run:
        try:
            provider = get_s3_provider()
            # ë²„í‚· í™•ì¸/ìƒì„±
            provider.ensure_bucket(bucket)
            logger.info(f"ðŸª£ ë²„í‚· ì¤€ë¹„ ì™„ë£Œ: {bucket}")
        except ImportError:
            print("âŒ boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install boto3")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ S3 ì—°ê²° ì‹¤íŒ¨: {e}")
            print("   AWS ìžê²© ì¦ëª…ì„ í™•ì¸í•˜ì„¸ìš”:")
            print("   - AWS_ACCESS_KEY_ID")
            print("   - AWS_SECRET_ACCESS_KEY")
            print("   - AWS_REGION")
            sys.exit(1)
    else:
        provider = None
        
    # ì—…ë¡œë“œ ì‹¤í–‰
    results = []
    total_size = 0
    
    if not args.no_db_update and data_type in ["episodes", "episode"]:
        register_episodes_in_db(files)

    for i, file_path in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] {file_path.name}")
        result = upload_file(
            provider,
            file_path,
            bucket,
            args.dry_run,
            prefix=prefix,
            data_type=data_type,
        )
        results.append(result)
        total_size += result.get("size_bytes", 0)
        
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ðŸ“Š ì—…ë¡œë“œ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    completed = sum(1 for r in results if r.get("status") == "completed")
    skipped = sum(1 for r in results if r.get("status") == "skipped")
    failed = sum(1 for r in results if r.get("status") in ["error", "failed"])
    dry_run_count = sum(1 for r in results if r.get("status") == "dry_run")
    
    print(f"  ì´ íŒŒì¼: {len(results)}ê°œ")
    print(f"  ì´ í¬ê¸°: {total_size / (1024*1024):.2f} MB")
    
    if args.dry_run:
        print(f"  ë¯¸ë¦¬ë³´ê¸°: {dry_run_count}ê°œ")
    else:
        print(f"  âœ… ì™„ë£Œ: {completed}ê°œ")
        print(f"  â­ï¸ ìŠ¤í‚µ: {skipped}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {failed}ê°œ")
        
        # DB ì—…ë°ì´íŠ¸
        if not args.no_db_update and (completed > 0 or skipped > 0):
            update_database(results)
    
    print()
    print("âœ… ì™„ë£Œ!")
    

if __name__ == "__main__":
    main()
