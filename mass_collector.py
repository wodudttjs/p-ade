#!/usr/bin/env python
"""
P-ADE ëŒ€ëŸ‰ ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

500ê°œ ì´ìƒì˜ ë¡œë´‡íŒ” ì˜ìƒì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” end-to-end íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
  1. í‚¤ì›Œë“œ ìƒì„± â†’ ë‹¤ì¤‘ ì†ŒìŠ¤ í¬ë¡¤ë§ (URL ìˆ˜ì§‘)
  2. ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (ë¹„ë””ì˜¤ íŒŒì¼ ì €ì¥)
  3. ê°ì²´ ê²€ì¶œ & Episode ìƒì„±
  4. S3 í´ë¼ìš°ë“œ ì—…ë¡œë“œ
  5. í†µê³„ ë¦¬í¬íŠ¸ ì¶œë ¥

ì‚¬ìš©ë²•:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (500ê°œ ëª©í‘œ)
    python mass_collector.py --target 500

    # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
    python mass_collector.py --target 500 --stage crawl

    # ë‹¤ìš´ë¡œë“œë¶€í„° ì¬ì‹œì‘
    python mass_collector.py --target 500 --stage download

    # íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰
    python mass_collector.py --stage detect --limit 100
    python mass_collector.py --stage upload

    # ì»¤ìŠ¤í…€ í‚¤ì›Œë“œë¡œ ì‹¤í–‰
    python mass_collector.py --target 200 --keywords "robot arm,pick and place,cobot"

    # ë“œë¼ì´ëŸ° (ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ê³„íšë§Œ ì¶œë ¥)
    python mass_collector.py --target 500 --dry-run
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field, asdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.logging_config import setup_logger
from config.settings import Config

logger = setup_logger(__name__)


# ============================================================
# íŒŒì´í”„ë¼ì¸ ì„¤ì •
# ============================================================

@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ìˆ˜ì§‘ ëª©í‘œ
    target_count: int = 500
    
    # í¬ë¡¤ë§ ì„¤ì •
    sources: List[str] = field(default_factory=lambda: ["youtube", "google_videos"])
    languages: List[str] = field(default_factory=lambda: ["en", "ko"])
    crawl_workers: int = 4
    crawl_full_info: bool = False
    min_duration_sec: int = 30
    max_duration_sec: int = 1200
    content_filter: bool = True
    
    # ë‹¤ìš´ë¡œë“œ ì„¤ì •
    download_workers: int = 6
    download_timeout: int = 600
    download_quality: str = "720p"
    
    # ê²€ì¶œ ì„¤ì •
    detect_fps: float = 5.0
    detect_device: Optional[str] = None  # None = auto-detect
    detect_batch_size: int = 50
    
    # ì—…ë¡œë“œ ì„¤ì •
    s3_bucket: str = ""
    s3_prefix: str = "episodes"
    upload_workers: int = 4
    
    # ê²½ë¡œ
    db_path: str = "data/pade.db"
    raw_dir: str = "data/raw"
    episodes_dir: str = "data/episodes"
    urls_csv: str = "data/urls_mass.csv"
    report_path: str = "data/collection_report.json"
    
    # ê¸°íƒ€
    dry_run: bool = False
    resume: bool = True  # ì´ì „ ì§„í–‰ ì´ì–´ë°›ê¸°
    
    @property
    def crawl_multiplier(self) -> float:
        """ëª©í‘œ ëŒ€ë¹„ í¬ë¡¤ë§ ì´ˆê³¼ ìˆ˜ì§‘ ë°°ìˆ˜ (í•„í„°ë§ ê°ì•ˆ)"""
        return 3.0
    
    @property
    def crawl_target(self) -> int:
        """ì‹¤ì œ í¬ë¡¤ë§ ëª©í‘œ (í•„í„°/ì¤‘ë³µ ê°ì•ˆ)"""
        return int(self.target_count * self.crawl_multiplier)


@dataclass
class StageResult:
    """ë‹¨ê³„ë³„ ê²°ê³¼"""
    stage: str
    success: bool
    count: int = 0
    errors: int = 0
    elapsed_sec: float = 0.0
    details: Dict[str, Any] = field(default_factory=dict)
    
    def summary(self) -> str:
        status = "âœ…" if self.success else "âŒ"
        return (
            f"{status} [{self.stage}] "
            f"ì™„ë£Œ: {self.count}ê°œ, ì˜¤ë¥˜: {self.errors}ê°œ, "
            f"ì†Œìš”: {self.elapsed_sec:.1f}ì´ˆ"
        )


@dataclass
class PipelineReport:
    """íŒŒì´í”„ë¼ì¸ ì „ì²´ ë¦¬í¬íŠ¸"""
    started_at: str = ""
    completed_at: str = ""
    config: Dict[str, Any] = field(default_factory=dict)
    stages: List[Dict[str, Any]] = field(default_factory=list)
    total_crawled: int = 0
    total_downloaded: int = 0
    total_episodes: int = 0
    total_uploaded: int = 0
    total_elapsed_sec: float = 0.0
    
    def save(self, path: str):
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(asdict(self), f, indent=2, ensure_ascii=False)
    
    def print_summary(self):
        print()
        print("=" * 70)
        print("ğŸ“‹ P-ADE ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë¦¬í¬íŠ¸")
        print("=" * 70)
        print(f"  ì‹œì‘: {self.started_at}")
        print(f"  ì™„ë£Œ: {self.completed_at}")
        print(f"  ì´ ì†Œìš”: {self.total_elapsed_sec:.1f}ì´ˆ ({self.total_elapsed_sec/60:.1f}ë¶„)")
        print()
        print(f"  ğŸ“Š ê²°ê³¼:")
        print(f"    í¬ë¡¤ë§ URL: {self.total_crawled}ê°œ")
        print(f"    ë‹¤ìš´ë¡œë“œ:   {self.total_downloaded}ê°œ")
        print(f"    ì—í”¼ì†Œë“œ:   {self.total_episodes}ê°œ")
        print(f"    ì—…ë¡œë“œ:     {self.total_uploaded}ê°œ")
        print()
        for stage in self.stages:
            sr = StageResult(**stage)
            print(f"  {sr.summary()}")
        print("=" * 70)


# ============================================================
# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸°
# ============================================================

class MassCollector:
    """ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    STAGES = ["crawl", "download", "detect", "upload"]

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.report = PipelineReport(
            started_at=datetime.now().isoformat(),
            config=asdict(config),
        )

    def run(self, start_stage: Optional[str] = None, end_stage: Optional[str] = None):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        stages = self.STAGES.copy()

        # ì‹œì‘/ì¢…ë£Œ ë‹¨ê³„ ì„¤ì •
        if start_stage:
            start_idx = stages.index(start_stage) if start_stage in stages else 0
            stages = stages[start_idx:]
        if end_stage:
            end_idx = stages.index(end_stage) + 1 if end_stage in stages else len(stages)
            stages = stages[:end_idx]

        total_start = time.time()

        print()
        print("ğŸš€" + "=" * 68)
        print("   P-ADE ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸")
        print("=" * 70)
        print(f"   ëª©í‘œ: {self.config.target_count}ê°œ ì˜ìƒ")
        print(f"   ì†ŒìŠ¤: {', '.join(self.config.sources)}")
        print(f"   ë‹¨ê³„: {' â†’ '.join(stages)}")
        print(f"   ë“œë¼ì´ëŸ°: {'ì˜ˆ' if self.config.dry_run else 'ì•„ë‹ˆì˜¤'}")
        print("=" * 70)

        for stage_name in stages:
            print(f"\n{'â”€'*70}")
            print(f"ğŸ“Œ ë‹¨ê³„: {stage_name.upper()}")
            print(f"{'â”€'*70}")

            handler = getattr(self, f"_stage_{stage_name}", None)
            if not handler:
                logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ê³„: {stage_name}")
                continue

            try:
                result = handler()
                self.report.stages.append(asdict(result))
                print(f"  {result.summary()}")
            except Exception as e:
                logger.error(f"ë‹¨ê³„ ì‹¤íŒ¨ [{stage_name}]: {e}")
                result = StageResult(
                    stage=stage_name, success=False, errors=1,
                    details={"error": str(e)},
                )
                self.report.stages.append(asdict(result))
                print(f"  {result.summary()}")
                # í¬ë¡¤ë§/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
                if stage_name in ("crawl",):
                    print("  âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨, ê¸°ì¡´ ë°ì´í„°ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

        self.report.total_elapsed_sec = time.time() - total_start
        self.report.completed_at = datetime.now().isoformat()
        self.report.save(self.config.report_path)
        self.report.print_summary()

    # ============================================================
    # 1ë‹¨ê³„: í¬ë¡¤ë§
    # ============================================================

    def _stage_crawl(self) -> StageResult:
        """í‚¤ì›Œë“œ ìƒì„± + ë‹¤ì¤‘ ì†ŒìŠ¤ í¬ë¡¤ë§"""
        from ingestion.keyword_generator import KeywordGenerator
        from ingestion.multi_source_crawler import MultiSourceCrawler

        start = time.time()

        # í‚¤ì›Œë“œ ìƒì„±
        gen = KeywordGenerator(
            languages=self.config.languages,
            max_keywords=200,
        )
        keywords = gen.get_flat_keywords(max_count=50)

        print(f"  ğŸ”‘ {len(keywords)}ê°œ í‚¤ì›Œë“œ ìƒì„±ë¨")
        for i, kw in enumerate(keywords[:10], 1):
            print(f"      {i}. {kw}")
        if len(keywords) > 10:
            print(f"      ... ì™¸ {len(keywords) - 10}ê°œ")

        if self.config.dry_run:
            return StageResult(
                stage="crawl",
                success=True,
                count=0,
                details={"keywords": len(keywords), "dry_run": True},
                elapsed_sec=time.time() - start,
            )

        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler = MultiSourceCrawler(
            sources=self.config.sources,
            max_results=self.config.crawl_target,
            max_workers=self.config.crawl_workers,
            get_full_info=self.config.crawl_full_info,
            min_duration_sec=self.config.min_duration_sec,
            max_duration_sec=self.config.max_duration_sec,
            content_filter=self.config.content_filter,
        )

        results, stats = crawler.crawl(keywords)

        # CSV ì €ì¥
        csv_path = Path(self.config.urls_csv)
        crawler.save_csv(results, csv_path, overwrite=True)

        # DB ì €ì¥
        saved = crawler.save_to_db(results, self.config.db_path)

        self.report.total_crawled = len(results)

        return StageResult(
            stage="crawl",
            success=len(results) > 0,
            count=len(results),
            errors=stats.total_errors,
            elapsed_sec=time.time() - start,
            details={
                "keywords_used": len(keywords),
                "db_saved": saved,
                "duplicates": stats.total_duplicates,
                "filtered": stats.total_filtered,
                "by_source": stats.by_source,
            },
        )

    # ============================================================
    # 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ
    # ============================================================

    def _stage_download(self) -> StageResult:
        """ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ"""
        import csv as csv_module

        start = time.time()
        csv_path = Path(self.config.urls_csv)
        output_dir = Path(self.config.raw_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # CSVì—ì„œ URL ë¡œë“œ
        videos = []
        if csv_path.exists():
            with csv_path.open("r", encoding="utf-8") as f:
                reader = csv_module.DictReader(f)
                for row in reader:
                    if row.get("url") and row.get("video_id"):
                        videos.append({
                            "video_id": row["video_id"],
                            "url": row["url"],
                            "title": row.get("title", ""),
                        })

        if not videos:
            # DBì—ì„œ ë¡œë“œ ì‹œë„
            videos = self._load_videos_from_db()

        if not videos:
            return StageResult(
                stage="download", success=False, errors=1,
                details={"error": "ë‹¤ìš´ë¡œë“œí•  URLì´ ì—†ìŠµë‹ˆë‹¤"},
                elapsed_sec=time.time() - start,
            )

        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸ (resume)
        if self.config.resume:
            existing = set(p.stem for p in output_dir.glob("*.mp4"))
            before = len(videos)
            videos = [v for v in videos if v["video_id"] not in existing]
            skipped = before - len(videos)
            if skipped > 0:
                print(f"  â­ï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: {skipped}ê°œ ìŠ¤í‚µ")

        # ëª©í‘œ ìˆ˜ë§Œí¼ë§Œ ë‹¤ìš´ë¡œë“œ
        videos = videos[:self.config.target_count]

        print(f"  ğŸ“¦ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ: {len(videos)}ê°œ")

        if self.config.dry_run:
            return StageResult(
                stage="download",
                success=True,
                count=0,
                details={"target": len(videos), "dry_run": True},
                elapsed_sec=time.time() - start,
            )

        # parallel_download ëª¨ë“ˆ ì‚¬ìš©
        from parallel_download import parallel_download, save_results_to_db

        results = parallel_download(
            videos=videos,
            output_dir=output_dir,
            num_workers=self.config.download_workers,
            timeout=self.config.download_timeout,
        )

        success_count = sum(1 for r in results if r.success and not r.skipped)
        skip_count = sum(1 for r in results if r.skipped)
        fail_count = sum(1 for r in results if not r.success)

        # DB ì—…ë°ì´íŠ¸
        db_saved = save_results_to_db(results, videos)

        self.report.total_downloaded = success_count + skip_count

        return StageResult(
            stage="download",
            success=success_count > 0,
            count=success_count,
            errors=fail_count,
            elapsed_sec=time.time() - start,
            details={
                "new_downloads": success_count,
                "skipped": skip_count,
                "failed": fail_count,
                "db_saved": db_saved,
            },
        )

    # ============================================================
    # 3ë‹¨ê³„: ê°ì²´ ê²€ì¶œ & Episode ìƒì„±
    # ============================================================

    def _stage_detect(self) -> StageResult:
        """ê°ì²´ ê²€ì¶œ ë° Episode ìƒì„±"""
        from extraction.detect_to_episodes import run as run_detect

        start = time.time()
        db_path = Path(self.config.db_path)
        output_dir = Path(self.config.episodes_dir)

        # ì´ë¯¸ ìƒì„±ëœ ì—í”¼ì†Œë“œ í™•ì¸
        existing_episodes = set()
        if output_dir.exists():
            existing_episodes = {p.stem.replace("_episode", "") for p in output_dir.glob("*.npz")}

        print(f"  ğŸ“‚ ê¸°ì¡´ ì—í”¼ì†Œë“œ: {len(existing_episodes)}ê°œ")

        if self.config.dry_run:
            return StageResult(
                stage="detect",
                success=True,
                count=0,
                details={"existing": len(existing_episodes), "dry_run": True},
                elapsed_sec=time.time() - start,
            )

        # detect_to_episodes ì‹¤í–‰
        try:
            run_detect(
                db_path=db_path,
                output_dir=output_dir,
                limit=self.config.target_count,
                use_redis=False,
                output_fps=self.config.detect_fps,
                device=self.config.detect_device,
            )
        except Exception as e:
            logger.error(f"ê²€ì¶œ ì‹¤íŒ¨: {e}")

        # ìƒˆë¡œ ìƒì„±ëœ ì—í”¼ì†Œë“œ í™•ì¸
        new_episodes = set()
        if output_dir.exists():
            new_episodes = {p.stem for p in output_dir.glob("*.npz")}
        
        new_count = len(new_episodes) - len(existing_episodes)
        total_episodes = len(new_episodes)

        self.report.total_episodes = total_episodes

        return StageResult(
            stage="detect",
            success=total_episodes > 0,
            count=new_count,
            elapsed_sec=time.time() - start,
            details={
                "total_episodes": total_episodes,
                "new_episodes": new_count,
                "device": self.config.detect_device or "auto",
            },
        )

    # ============================================================
    # 4ë‹¨ê³„: S3 ì—…ë¡œë“œ
    # ============================================================

    def _stage_upload(self) -> StageResult:
        """S3 ì—…ë¡œë“œ"""
        start = time.time()
        episodes_dir = Path(self.config.episodes_dir)

        if not episodes_dir.exists():
            return StageResult(
                stage="upload", success=False, errors=1,
                details={"error": "ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤"},
                elapsed_sec=time.time() - start,
            )

        npz_files = list(episodes_dir.glob("*.npz"))
        print(f"  ğŸ“‚ ì—…ë¡œë“œ ëŒ€ìƒ: {len(npz_files)}ê°œ íŒŒì¼")

        if not npz_files:
            return StageResult(
                stage="upload", success=True, count=0,
                details={"message": "ì—…ë¡œë“œí•  íŒŒì¼ ì—†ìŒ"},
                elapsed_sec=time.time() - start,
            )

        if self.config.dry_run:
            total_size = sum(f.stat().st_size for f in npz_files)
            return StageResult(
                stage="upload",
                success=True,
                count=0,
                details={
                    "files": len(npz_files),
                    "total_size_mb": total_size / (1024 ** 2),
                    "dry_run": True,
                },
                elapsed_sec=time.time() - start,
            )

        # upload_to_s3 ëª¨ë“ˆ ì‚¬ìš©
        try:
            from upload_to_s3 import get_s3_provider, get_bucket_name, upload_file

            provider = get_s3_provider()
            bucket = self.config.s3_bucket or get_bucket_name()

            uploaded = 0
            errors = 0

            for npz_file in npz_files:
                try:
                    result = upload_file(
                        provider=provider,
                        local_path=npz_file,
                        bucket=bucket,
                        prefix=self.config.s3_prefix,
                        data_type="episode",
                    )
                    if result.get("status") in ("uploaded", "completed"):
                        uploaded += 1
                    elif result.get("status") == "skipped":
                        uploaded += 1  # ì´ë¯¸ ì¡´ì¬
                except Exception as e:
                    errors += 1
                    logger.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {npz_file.name} - {e}")

            self.report.total_uploaded = uploaded

            return StageResult(
                stage="upload",
                success=uploaded > 0,
                count=uploaded,
                errors=errors,
                elapsed_sec=time.time() - start,
                details={"bucket": bucket, "prefix": self.config.s3_prefix},
            )

        except Exception as e:
            return StageResult(
                stage="upload", success=False, errors=1,
                details={"error": str(e)},
                elapsed_sec=time.time() - start,
            )

    # ============================================================
    # ìœ í‹¸ë¦¬í‹°
    # ============================================================

    def _load_videos_from_db(self) -> List[Dict]:
        """DBì—ì„œ discovered ìƒíƒœ ë¹„ë””ì˜¤ ë¡œë“œ"""
        try:
            from sqlalchemy import create_engine
            from sqlalchemy.orm import sessionmaker
            from models.database import Base, Video

            db_file = Path(self.config.db_path)
            if not db_file.is_absolute():
                db_file = project_root / db_file

            engine = create_engine(f"sqlite:///{db_file}")
            Session = sessionmaker(bind=engine)
            session = Session()

            videos = session.query(Video).filter(
                Video.status == "discovered",
                Video.url.isnot(None),
            ).limit(self.config.target_count).all()

            result = [
                {
                    "video_id": v.video_id,
                    "url": v.url,
                    "title": v.title or "",
                }
                for v in videos
            ]
            session.close()
            return result

        except Exception as e:
            logger.error(f"DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="P-ADE ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆ:
  # ì „ì²´ íŒŒì´í”„ë¼ì¸ (500ê°œ ëª©í‘œ)
  python mass_collector.py --target 500

  # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
  python mass_collector.py --target 500 --stage crawl

  # ë‹¤ìš´ë¡œë“œë¶€í„° ì¬ì‹œì‘
  python mass_collector.py --target 500 --start-stage download

  # íŠ¹ì • ë‹¨ê³„ êµ¬ê°„ ì‹¤í–‰
  python mass_collector.py --start-stage download --end-stage detect

  # ë“œë¼ì´ëŸ°
  python mass_collector.py --target 500 --dry-run

  # ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ
  python mass_collector.py --target 200 --keywords "robot arm,pick and place"

  # ì†ŒìŠ¤ ì§€ì •
  python mass_collector.py --target 500 --sources youtube,google_videos,vimeo
        """
    )

    parser.add_argument("--target", type=int, default=500, help="ìˆ˜ì§‘ ëª©í‘œ ìˆ˜ (ê¸°ë³¸: 500)")
    parser.add_argument("--stage", help="ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰: crawl, download, detect, upload")
    parser.add_argument("--start-stage", help="ì‹œì‘ ë‹¨ê³„")
    parser.add_argument("--end-stage", help="ì¢…ë£Œ ë‹¨ê³„")
    parser.add_argument("--keywords", help="ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ (ì½¤ë§ˆ êµ¬ë¶„)")
    parser.add_argument("--sources", default="youtube,google_videos",
                       help="ì†ŒìŠ¤ (ê¸°ë³¸: youtube,google_videos)")
    parser.add_argument("--languages", default="en,ko", help="í‚¤ì›Œë“œ ì–¸ì–´ (ê¸°ë³¸: en,ko)")
    parser.add_argument("--crawl-workers", type=int, default=4)
    parser.add_argument("--download-workers", type=int, default=6)
    parser.add_argument("--download-timeout", type=int, default=600)
    parser.add_argument("--detect-fps", type=float, default=5.0)
    parser.add_argument("--detect-device", default=None, help="ê²€ì¶œ ë””ë°”ì´ìŠ¤ (ì˜ˆ: cuda:0)")
    parser.add_argument("--s3-bucket", default="")
    parser.add_argument("--s3-prefix", default="episodes")
    parser.add_argument("--db", default="data/pade.db")
    parser.add_argument("--output-dir", default="data/raw")
    parser.add_argument("--episodes-dir", default="data/episodes")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ê³„íšë§Œ ì¶œë ¥")
    parser.add_argument("--no-resume", action="store_true", help="ì´ì „ ì§„í–‰ ë¬´ì‹œ")
    parser.add_argument("--min-duration", type=int, default=30)
    parser.add_argument("--max-duration", type=int, default=1200)

    args = parser.parse_args()

    config = PipelineConfig(
        target_count=args.target,
        sources=[s.strip() for s in args.sources.split(",")],
        languages=[l.strip() for l in args.languages.split(",")],
        crawl_workers=args.crawl_workers,
        min_duration_sec=args.min_duration,
        max_duration_sec=args.max_duration,
        download_workers=args.download_workers,
        download_timeout=args.download_timeout,
        detect_fps=args.detect_fps,
        detect_device=args.detect_device,
        s3_bucket=args.s3_bucket,
        s3_prefix=args.s3_prefix,
        db_path=args.db,
        raw_dir=args.output_dir,
        episodes_dir=args.episodes_dir,
        dry_run=args.dry_run,
        resume=not args.no_resume,
    )

    collector = MassCollector(config)

    if args.stage:
        # ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰
        collector.run(start_stage=args.stage, end_stage=args.stage)
    elif args.start_stage or args.end_stage:
        collector.run(start_stage=args.start_stage, end_stage=args.end_stage)
    else:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        collector.run()


if __name__ == "__main__":
    main()
