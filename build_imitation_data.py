#!/usr/bin/env python3
"""
ëª¨ë°©í•™ìŠµ ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸

ë¹„ë””ì˜¤ â†’ í¬ì¦ˆ ì¶”ì¶œ(MediaPipe Tasks API) â†’ State-Action ì¸ì½”ë”© â†’ .npz ì €ì¥

ìƒì„±ë˜ëŠ” ë°ì´í„° êµ¬ì¡°:
  - states:       [T, state_dim]   ì •ê·œí™”ëœ ê´€ì ˆ ìœ„ì¹˜ + ì†ë„
  - actions:      [T-1, action_dim] í”„ë ˆì„ ê°„ ìœ„ì¹˜ ë³€í™”(delta)
  - poses:        [T, 33, 3]       ì •ê·œí™”ëœ ê´€ì ˆ ì¢Œí‘œ
  - velocity:     [T, 33, 3]       ê´€ì ˆ ì†ë„
  - left_hand:    [T, 21, 3]       ì™¼ì† ëœë“œë§ˆí¬
  - right_hand:   [T, 21, 3]       ì˜¤ë¥¸ì† ëœë“œë§ˆí¬
  - timestamps:   [T]              íƒ€ì„ìŠ¤íƒ¬í”„
  - confidence:   [T]              í¬ì¦ˆ ì‹ ë¢°ë„
  - gripper_state:[T]              ê·¸ë¦¬í¼(ì† ì˜¤ë¯€ë¦¼) ìƒíƒœ ì¶”ì •
"""

import os
import sys
import argparse
import time
import json
import traceback
from pathlib import Path
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

import cv2
import numpy as np

PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

# ============================================================================
# MediaPipe í¬ì¦ˆ ì¶”ì¶œ (Tasks API - 0.10.x)
# ============================================================================

def extract_pose_from_video(video_path: str, output_fps: float = 5.0, max_frames: int = None):
    """
    MediaPipe Tasks APIë¡œ ë¹„ë””ì˜¤ì—ì„œ í¬ì¦ˆ+ì† ì¶”ì¶œ
    
    Returns:
        dict with body[T,33,3], body_world[T,33,3],
             left_hand[T,21,3], right_hand[T,21,3],
             timestamps[T], confidence[T], fps
    """
    import mediapipe as mp
    from mediapipe.tasks.python import vision
    from mediapipe.tasks.python.core import base_options as mp_base

    model_path = str(PROJECT_ROOT / "models" / "mediapipe" / "pose_landmarker.task")
    hand_model_path = str(PROJECT_ROOT / "models" / "mediapipe" / "hand_landmarker.task")

    # --- Pose Landmarker ---
    pose_options = vision.PoseLandmarkerOptions(
        base_options=mp_base.BaseOptions(model_asset_path=model_path),
        running_mode=vision.RunningMode.VIDEO,
        num_poses=1,
        min_pose_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        output_segmentation_masks=False,
    )
    pose_landmarker = vision.PoseLandmarker.create_from_options(pose_options)

    # --- Hand Landmarker ---
    hand_landmarker = None
    if Path(hand_model_path).exists():
        hand_options = vision.HandLandmarkerOptions(
            base_options=mp_base.BaseOptions(model_asset_path=hand_model_path),
            running_mode=vision.RunningMode.VIDEO,
            num_hands=2,
            min_hand_detection_confidence=0.5,
            min_tracking_confidence=0.5,
        )
        hand_landmarker = vision.HandLandmarker.create_from_options(hand_options)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Cannot open video: {video_path}")

    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_interval = max(1, int(orig_fps / output_fps))

    body_list = []
    body_world_list = []
    left_hand_list = []
    right_hand_list = []
    timestamps = []
    confidences = []

    frame_idx = 0
    processed = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_interval != 0:
            frame_idx += 1
            continue

        if max_frames and processed >= max_frames:
            break

        timestamp_ms = int((frame_idx / orig_fps) * 1000)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)

        # í¬ì¦ˆ ê²€ì¶œ
        try:
            pose_result = pose_landmarker.detect_for_video(mp_image, timestamp_ms)
        except Exception:
            frame_idx += 1
            continue

        # í¬ì¦ˆ ë°ì´í„° ì¶”ì¶œ
        if pose_result.pose_landmarks and len(pose_result.pose_landmarks) > 0:
            lms = pose_result.pose_landmarks[0]
            body = np.array([[l.x, l.y, l.z] for l in lms])
            conf = np.mean([l.visibility for l in lms]) if hasattr(lms[0], 'visibility') else 0.5
        else:
            body = np.zeros((33, 3))
            conf = 0.0

        # ì›”ë“œ ì¢Œí‘œ
        if pose_result.pose_world_landmarks and len(pose_result.pose_world_landmarks) > 0:
            wlms = pose_result.pose_world_landmarks[0]
            body_w = np.array([[l.x, l.y, l.z] for l in wlms])
        else:
            body_w = np.zeros((33, 3))

        # ì† ê²€ì¶œ
        lh = np.zeros((21, 3))
        rh = np.zeros((21, 3))
        if hand_landmarker:
            try:
                hand_result = hand_landmarker.detect_for_video(mp_image, timestamp_ms)
                if hand_result.hand_landmarks:
                    for i, hand_lms in enumerate(hand_result.hand_landmarks):
                        hand_arr = np.array([[l.x, l.y, l.z] for l in hand_lms])
                        # handedness í™•ì¸
                        if hand_result.handedness and i < len(hand_result.handedness):
                            label = hand_result.handedness[i][0].category_name.lower()
                            if label == 'left':
                                lh = hand_arr
                            else:
                                rh = hand_arr
                        else:
                            if i == 0:
                                rh = hand_arr
                            else:
                                lh = hand_arr
            except Exception:
                pass

        body_list.append(body)
        body_world_list.append(body_w)
        left_hand_list.append(lh)
        right_hand_list.append(rh)
        timestamps.append(frame_idx / orig_fps)
        confidences.append(conf)

        frame_idx += 1
        processed += 1

    cap.release()
    pose_landmarker.close()
    if hand_landmarker:
        hand_landmarker.close()

    if not body_list:
        return None

    return {
        "body": np.array(body_list, dtype=np.float32),           # [T, 33, 3]
        "body_world": np.array(body_world_list, dtype=np.float32), # [T, 33, 3]
        "left_hand": np.array(left_hand_list, dtype=np.float32),   # [T, 21, 3]
        "right_hand": np.array(right_hand_list, dtype=np.float32), # [T, 21, 3]
        "timestamps": np.array(timestamps, dtype=np.float32),      # [T]
        "confidence": np.array(confidences, dtype=np.float32),     # [T]
        "fps": output_fps,
    }


# ============================================================================
# ëª¨ë°©í•™ìŠµ ë°ì´í„° ì¸ì½”ë”©
# ============================================================================

def normalize_poses(poses: np.ndarray) -> np.ndarray:
    """í¬ì¦ˆ ì •ê·œí™”: hip ì¤‘ì‹¬, ì–´ê¹¨ ë„ˆë¹„ scale"""
    # Hip center (23=left_hip, 24=right_hip)
    hip_center = (poses[:, 23, :] + poses[:, 24, :]) / 2
    normalized = poses - hip_center[:, np.newaxis, :]
    
    # ì–´ê¹¨ ë„ˆë¹„ ê¸°ì¤€ ìŠ¤ì¼€ì¼ë§
    shoulder_width = np.linalg.norm(poses[:, 11, :] - poses[:, 12, :], axis=1)
    scale = np.clip(shoulder_width, 0.01, 2.0)
    normalized = normalized / scale[:, np.newaxis, np.newaxis]
    
    return normalized


def compute_velocity(poses: np.ndarray, fps: float) -> np.ndarray:
    """ì¤‘ì•™ ì°¨ë¶„ ì†ë„ ê³„ì‚°"""
    dt = 1.0 / fps
    vel = np.zeros_like(poses)
    if len(poses) > 2:
        vel[1:-1] = (poses[2:] - poses[:-2]) / (2 * dt)
        vel[0] = (poses[1] - poses[0]) / dt
        vel[-1] = (poses[-1] - poses[-2]) / dt
    elif len(poses) == 2:
        v = (poses[1] - poses[0]) / dt
        vel[0] = v
        vel[1] = v
    return vel


def estimate_gripper_state(left_hand: np.ndarray, right_hand: np.ndarray) -> np.ndarray:
    """
    ì† ì˜¤ë¯€ë¦¼ ì •ë„ë¡œ ê·¸ë¦¬í¼ ìƒíƒœ ì¶”ì •
    0.0 = ì™„ì „ ì—´ë¦¼, 1.0 = ì™„ì „ ë‹«í˜(ì¥ê¸°)
    ì˜¤ë¥¸ì† ê¸°ì¤€ (ë¡œë´‡íŒ” end-effector)
    """
    T = len(right_hand)
    gripper = np.zeros(T, dtype=np.float32)
    
    for t in range(T):
        hand = right_hand[t]
        if np.all(hand == 0):
            # ì† ë¯¸ê²€ì¶œ â†’ ì™¼ì† ì‹œë„
            hand = left_hand[t]
            if np.all(hand == 0):
                gripper[t] = 0.5  # ë¶ˆí™•ì‹¤
                continue
        
        # ì†ê°€ë½ ë(4,8,12,16,20)ê³¼ ì†ë°”ë‹¥(0) ì‚¬ì´ ê±°ë¦¬
        palm = hand[0]
        fingertips = hand[[4, 8, 12, 16, 20]]
        distances = np.linalg.norm(fingertips - palm, axis=1)
        avg_dist = np.mean(distances)
        
        # ì •ê·œí™” (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ë‹«í˜)
        # ì¼ë°˜ì ìœ¼ë¡œ ì—´ë¦° ì†: 0.15~0.25, ë‹«íŒ ì†: 0.03~0.08
        gripper[t] = np.clip(1.0 - (avg_dist - 0.03) / 0.20, 0.0, 1.0)
    
    return gripper


def build_states(norm_poses: np.ndarray, velocity: np.ndarray, 
                 confidence: np.ndarray) -> np.ndarray:
    """
    State ë²¡í„° ìƒì„±: [ê´€ì ˆìœ„ì¹˜ flat | ê´€ì ˆì†ë„ flat | ì‹ ë¢°ë„]
    """
    T = norm_poses.shape[0]
    pos_flat = norm_poses.reshape(T, -1)    # [T, 99]
    vel_flat = velocity.reshape(T, -1)      # [T, 99]
    conf = confidence.reshape(T, 1)          # [T, 1]
    
    states = np.concatenate([pos_flat, vel_flat, conf], axis=1)  # [T, 199]
    return states.astype(np.float32)


def build_actions(norm_poses: np.ndarray, fps: float, 
                  gripper: np.ndarray) -> np.ndarray:
    """
    Action ë²¡í„° ìƒì„±: [ê´€ì ˆìœ„ì¹˜ delta flat | gripper_state]
    delta = (pose[t+1] - pose[t]) * fps
    """
    T = norm_poses.shape[0]
    
    # ìœ„ì¹˜ ë³€í™”ëŸ‰
    delta = np.diff(norm_poses, axis=0) * fps  # [T-1, 33, 3]
    delta_flat = delta.reshape(T - 1, -1)       # [T-1, 99]
    
    # ê·¸ë¦¬í¼ ìƒíƒœ (t+1 ê¸°ì¤€)
    grip = gripper[1:].reshape(T - 1, 1)
    
    actions = np.concatenate([delta_flat, grip], axis=1)  # [T-1, 100]
    return actions.astype(np.float32)


def encode_imitation_data(pose_data: dict, video_id: str) -> dict:
    """í¬ì¦ˆ ë°ì´í„° â†’ ëª¨ë°©í•™ìŠµ ë°ì´í„° ì¸ì½”ë”©"""
    body = pose_data["body"]           # [T, 33, 3]
    body_world = pose_data["body_world"]
    left_hand = pose_data["left_hand"]
    right_hand = pose_data["right_hand"]
    timestamps = pose_data["timestamps"]
    confidence = pose_data["confidence"]
    fps = pose_data["fps"]
    T = body.shape[0]
    
    if T < 3:
        raise ValueError(f"í”„ë ˆì„ ìˆ˜ ë¶€ì¡±: {T}")
    
    # 1) í¬ì¦ˆ ì •ê·œí™”
    norm_poses = normalize_poses(body)
    
    # 2) ì†ë„ ê³„ì‚°
    velocity = compute_velocity(norm_poses, fps)
    
    # 3) ê·¸ë¦¬í¼ ìƒíƒœ ì¶”ì •
    gripper = estimate_gripper_state(left_hand, right_hand)
    
    # 4) State ë²¡í„° ìƒì„±
    states = build_states(norm_poses, velocity, confidence)
    
    # 5) Action ë²¡í„° ìƒì„±
    actions = build_actions(norm_poses, fps, gripper)
    
    return {
        # í•µì‹¬ ëª¨ë°©í•™ìŠµ ë°ì´í„°
        "states": states,                          # [T, 199]
        "actions": actions,                        # [T-1, 100]
        
        # ì›ì‹œ í¬ì¦ˆ ë°ì´í„°
        "poses": norm_poses.astype(np.float32),    # [T, 33, 3]
        "poses_raw": body.astype(np.float32),      # [T, 33, 3]
        "poses_world": body_world.astype(np.float32),
        "velocity": velocity.astype(np.float32),   # [T, 33, 3]
        
        # ì† ë°ì´í„°
        "left_hand": left_hand.astype(np.float32), # [T, 21, 3]
        "right_hand": right_hand.astype(np.float32),
        
        # ê·¸ë¦¬í¼ & ë©”íƒ€
        "gripper_state": gripper,                   # [T]
        "timestamps": timestamps,                   # [T]
        "confidence": confidence,                   # [T]
        
        # ë©”íƒ€ë°ì´í„°
        "fps": np.float32(fps),
        "video_id": str(video_id),
        "num_frames": np.int32(T),
        "state_dim": np.int32(states.shape[1]),
        "action_dim": np.int32(actions.shape[1]),
        "created_at": datetime.now().isoformat(),
    }


# ============================================================================
# ë‹¨ì¼ ë¹„ë””ì˜¤ ì²˜ë¦¬ (í”„ë¡œì„¸ìŠ¤ í’€ìš©)
# ============================================================================

def process_single_video(args_tuple):
    """ë‹¨ì¼ ë¹„ë””ì˜¤: í¬ì¦ˆ ì¶”ì¶œ â†’ ì¸ì½”ë”© â†’ ì €ì¥"""
    video_path, output_dir, output_fps, max_frames, idx, total = args_tuple
    video_id = Path(video_path).stem
    output_path = Path(output_dir) / f"{video_id}_episode.npz"
    
    # ì´ë¯¸ ëª¨ë°©í•™ìŠµ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if output_path.exists():
        try:
            d = np.load(output_path, allow_pickle=True)
            if "states" in d and "actions" in d:
                return {"video_id": video_id, "status": "skipped", "msg": "already has IL data"}
        except:
            pass
    
    start = time.time()
    try:
        # 1) í¬ì¦ˆ ì¶”ì¶œ
        pose_data = extract_pose_from_video(str(video_path), output_fps, max_frames)
        if pose_data is None:
            return {"video_id": video_id, "status": "failed", "msg": "no pose detected"}
        
        T = pose_data["body"].shape[0]
        if T < 3:
            return {"video_id": video_id, "status": "failed", "msg": f"too few frames: {T}"}
        
        # 2) ëª¨ë°©í•™ìŠµ ë°ì´í„° ì¸ì½”ë”©
        il_data = encode_imitation_data(pose_data, video_id)
        
        # 3) ì €ì¥
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        np.savez_compressed(output_path, **il_data)
        
        elapsed = time.time() - start
        return {
            "video_id": video_id,
            "status": "success",
            "frames": int(T),
            "state_dim": int(il_data["state_dim"]),
            "action_dim": int(il_data["action_dim"]),
            "time": round(elapsed, 1),
        }
    except Exception as e:
        elapsed = time.time() - start
        return {
            "video_id": video_id,
            "status": "failed",
            "msg": str(e),
            "time": round(elapsed, 1),
        }


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="ëª¨ë°©í•™ìŠµ ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--input-dir", default="data/raw", help="ë¹„ë””ì˜¤ ì…ë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--output-dir", default="data/episodes", help="ì—í”¼ì†Œë“œ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--fps", type=float, default=5.0, help="ì¶”ì¶œ FPS (ê¸°ë³¸ 5)")
    parser.add_argument("--max-frames", type=int, default=None, help="ë¹„ë””ì˜¤ë‹¹ ìµœëŒ€ í”„ë ˆì„")
    parser.add_argument("--limit", type=int, default=None, help="ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ìˆ˜ ì œí•œ")
    parser.add_argument("--workers", type=int, default=1, help="ë³‘ë ¬ ì›Œì»¤ ìˆ˜")
    args = parser.parse_args()
    
    input_dir = PROJECT_ROOT / args.input_dir
    output_dir = PROJECT_ROOT / args.output_dir
    
    videos = sorted(input_dir.glob("*.mp4"))
    if not videos:
        print("âŒ ë¹„ë””ì˜¤ ì—†ìŒ")
        return 1
    
    if args.limit:
        videos = videos[:args.limit]
    
    print(f"\n{'='*60}")
    print(f"ğŸ¤– ëª¨ë°©í•™ìŠµ ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸")
    print(f"{'='*60}")
    print(f"ğŸ“¹ ë¹„ë””ì˜¤: {len(videos)}ê°œ")
    print(f"ğŸ“ ì¶œë ¥:   {output_dir}")
    print(f"ğŸ¯ FPS:    {args.fps}")
    print(f"âš™ï¸  ì›Œì»¤:   {args.workers}")
    print(f"{'='*60}\n")
    
    tasks = [
        (str(v), str(output_dir), args.fps, args.max_frames, i, len(videos))
        for i, v in enumerate(videos)
    ]
    
    results = []
    success = 0
    failed = 0
    skipped = 0
    
    start_all = time.time()
    
    # ìˆœì°¨ ì²˜ë¦¬ (MediaPipeëŠ” í”„ë¡œì„¸ìŠ¤ë³„ ëª¨ë¸ ë¡œë”© í•„ìš”)
    for i, task in enumerate(tasks, 1):
        vid = Path(task[0]).stem
        print(f"[{i}/{len(tasks)}] {vid}...", end=" ", flush=True)
        
        result = process_single_video(task)
        results.append(result)
        
        if result["status"] == "success":
            success += 1
            print(f"âœ… {result['frames']}f S:{result['state_dim']} A:{result['action_dim']} ({result['time']}s)")
        elif result["status"] == "skipped":
            skipped += 1
            print(f"â­ï¸  {result['msg']}")
        else:
            failed += 1
            print(f"âŒ {result.get('msg','unknown')}")
    
    elapsed = time.time() - start_all
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"âœ… ì„±ê³µ: {success}")
    print(f"â­ï¸  ìŠ¤í‚µ: {skipped}")
    print(f"âŒ ì‹¤íŒ¨: {failed}")
    print(f"â±ï¸  ì†Œìš”: {elapsed:.1f}s")
    print(f"{'='*60}")
    
    # ê²°ê³¼ ê²€ì¦
    if success > 0:
        print(f"\nğŸ” ë°ì´í„° ê²€ì¦...")
        sample = list(Path(output_dir).glob("*_episode.npz"))
        if sample:
            d = np.load(sample[0], allow_pickle=True)
            print(f"  íŒŒì¼: {sample[0].name}")
            print(f"  í‚¤:   {list(d.keys())}")
            for k in ["states", "actions", "poses", "velocity", "gripper_state"]:
                if k in d:
                    print(f"  âœ… {k}: shape={d[k].shape}")
                else:
                    print(f"  âŒ {k}: ì—†ìŒ")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
